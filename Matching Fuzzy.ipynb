{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c315d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibliotecas Baixadas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     C:\\Users\\caios\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\caios\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score, v_measure_score\n",
    "from sklearn.datasets import load_files\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "from mpl_toolkits import mplot3d\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "print(\"Bibliotecas Baixadas\")\n",
    "\n",
    "\n",
    "nltk.download('rslp')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0003bae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregue os DataFrames origem_ID e text do CSV\n",
    "caminho_arquivo_1= 'cod_material_servico.txt'\n",
    "origem_ID = pd.read_csv(caminho_arquivo_1,sep=\";\" )\n",
    "origem_ID['ID'] = range(1, len(origem_ID) + 1)\n",
    "\n",
    "caminho_arquivo_2= 'text.txt'\n",
    "text = pd.read_csv(caminho_arquivo_2,sep=\",\" )\n",
    "text['ID'] = range(1, len(text) + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f44110",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779eb7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limpando text.txt\n",
    "\n",
    "text['dsc_item'] = text['dsc_item'].astype(str)\n",
    "origem_ID['classif'] = origem_ID['classif'].astype(str)\n",
    "text[\"dsc_item\"] = text[\"dsc_item\"].apply(lambda x: x.lower()) #converte todas as letras para minúsculo\n",
    "origem_ID['classif'] = origem_ID['classif'].apply(lambda x: x.lower()) #converte todas as letras para minúsculo \n",
    "text[\"dsc_item\"] = text[\"dsc_item\"].apply(lambda x: re.sub('|,|\\.|/|$|\\(|\\)|-|\\+|:|•', '', x)) #remove números e caracteres especiais \n",
    "origem_ID['classif'] = origem_ID['classif'].apply(lambda x: re.sub('|,|\\.|/|$|\\(|\\)|-|\\+|:|•', '', x)) #remove números e caracteres especiais \n",
    "text[\"dsc_item\"] = text[\"dsc_item\"].apply(lambda x: unidecode(x)) #remove acentos\n",
    "origem_ID['classif'] = origem_ID['classif'].apply(lambda x: unidecode(x)) #remove acentos\n",
    "\n",
    "stemmer = nltk.stem.RSLPStemmer() #converte as palavras para seu radical \n",
    "text[\"dsc_item\"] = text[\"dsc_item\"].apply(lambda x: stemmer.stem(x))\n",
    "\n",
    "stemmer1 = nltk.stem.RSLPStemmer() #converte as palavras para seu radical \n",
    "origem_ID['classif'] = origem_ID['classif'].apply(lambda x: stemmer.stem(x))\n",
    "\n",
    "\n",
    "# Função para encontrar o correspondente com o maior score em 'classif' para um valor de 'dsc_item'\n",
    "def find_best_match(dsc_item_value):\n",
    "    matches = process.extractOne(dsc_item_value, origem_ID['classif'], scorer=fuzz.ratio)\n",
    "    return matches\n",
    "\n",
    "\n",
    "# Encontre o correspondente com o maior score em 'classif' para cada valor único em 'dsc_item'\n",
    "unique_dsc_item_values = text['dsc_item'].unique()\n",
    "best_matches = {}\n",
    "\n",
    "for dsc_item_value in unique_dsc_item_values:\n",
    "    best_match = find_best_match(dsc_item_value)\n",
    "    best_matches[dsc_item_value] = best_match\n",
    "\n",
    "# Crie um DataFrame com os resultados, incluindo as colunas 'ID_classif' e 'ID_dsc_item'\n",
    "#result_df = pd.DataFrame(list(best_matches.values()), index=list(best_matches.keys()), columns=['melhor_classif', 'score'])\n",
    "result_df = pd.DataFrame(list(best_matches.values()), index=list(best_matches.keys()), columns=['melhor_classif', 'score', 'additional_identifier'])\n",
    "\n",
    "result_df['dsc_item'] = result_df.index\n",
    "result_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Adicione as colunas 'ID_classif' e 'ID_dsc_item'\n",
    "result_df['ID_classif'] = origem_ID.set_index('classif').loc[result_df['melhor_classif'], 'ID'].values\n",
    "result_df['ID_dsc_item'] = text.set_index('dsc_item').loc[result_df['dsc_item'], 'ID'].values\n",
    "\n",
    "# Reordene as colunas para ter 'dsc_item' na primeira posição\n",
    "result_df = result_df[['dsc_item', 'melhor_classif', 'score', 'ID_classif', 'ID_dsc_item']]\n",
    "\n",
    "# Salvar o DataFrame resultante em um novo CSV\n",
    "result_df.to_csv('resultado.csv', index=False)\n",
    "\n",
    "# Exibir as primeiras linhas do DataFrame resultante\n",
    "print(result_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f12d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score, v_measure_score\n",
    "from sklearn.datasets import load_files\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "from mpl_toolkits import mplot3d\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "print(\"Bibliotecas Baixadas\")\n",
    "\n",
    "\n",
    "nltk.download('rslp')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Carregue os DataFrames origem_ID e text do CSV\n",
    "caminho_arquivo_1= 'cod_material_servico.txt'\n",
    "origem_ID = pd.read_csv(caminho_arquivo_1,sep=\";\" )\n",
    "origem_ID['ID'] = range(1, len(origem_ID) + 1)\n",
    "\n",
    "caminho_arquivo_2= 'text.txt'\n",
    "text = pd.read_csv(caminho_arquivo_2,sep=\",\" )\n",
    "text['ID'] = range(1, len(text) + 1)\n",
    "\n",
    "\n",
    "text['dsc_item'] = text['dsc_item'].astype(str)\n",
    "origem_ID['classif'] = origem_ID['classif'].astype(str)\n",
    "text[\"dsc_item\"] = text[\"dsc_item\"].apply(lambda x: x.lower()) #converte todas as letras para minúsculo\n",
    "origem_ID['classif'] = origem_ID['classif'].apply(lambda x: x.lower()) #converte todas as letras para minúsculo \n",
    "text[\"dsc_item\"] = text[\"dsc_item\"].apply(lambda x: re.sub('|,|\\.|/|$|\\(|\\)|-|\\+|:|•', '', x)) #remove números e caracteres especiais \n",
    "origem_ID['classif'] = origem_ID['classif'].apply(lambda x: re.sub('|,|\\.|/|$|\\(|\\)|-|\\+|:|•', '', x)) #remove números e caracteres especiais \n",
    "text[\"dsc_item\"] = text[\"dsc_item\"].apply(lambda x: unidecode(x)) #remove acentos\n",
    "origem_ID['classif'] = origem_ID['classif'].apply(lambda x: unidecode(x)) #remove acentos\n",
    "\n",
    "stemmer = nltk.stem.RSLPStemmer() #converte as palavras para seu radical \n",
    "text[\"dsc_item\"] = text[\"dsc_item\"].apply(lambda x: stemmer.stem(x))\n",
    "\n",
    "stemmer1 = nltk.stem.RSLPStemmer() #converte as palavras para seu radical \n",
    "origem_ID['classif'] = origem_ID['classif'].apply(lambda x: stemmer.stem(x))\n",
    "\n",
    "\n",
    "# Função para encontrar o correspondente com o maior score em 'classif' para um valor de 'dsc_item'\n",
    "def find_best_match(dsc_item_value):\n",
    "    matches = process.extractOne(dsc_item_value, origem_ID['classif'], scorer=fuzz.ratio)\n",
    "    return matches\n",
    "\n",
    "# Encontre o correspondente com o maior score em 'classif' para cada valor único em 'dsc_item'\n",
    "unique_dsc_item_values = text['dsc_item'].unique()\n",
    "best_matches = {}\n",
    "\n",
    "for dsc_item_value in unique_dsc_item_values:\n",
    "    best_match = find_best_match(dsc_item_value)\n",
    "    best_matches[dsc_item_value] = best_match\n",
    "\n",
    "# Crie um DataFrame com os resultados, incluindo as colunas 'ID_classif' e 'ID_dsc_item'\n",
    "result_df = pd.DataFrame(list(best_matches.values()), index=list(best_matches.keys()), columns=['melhor_classif', 'score'])\n",
    "result_df['dsc_item'] = result_df.index\n",
    "result_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Adicione as colunas 'ID_classif' e 'ID_dsc_item'\n",
    "result_df['ID_classif'] = origem_ID.set_index('classif').loc[result_df['melhor_classif'], 'ID'].values\n",
    "result_df['ID_dsc_item'] = text.set_index('dsc_item').loc[result_df['dsc_item'], 'ID'].values\n",
    "\n",
    "# Reordene as colunas para ter 'dsc_item' na primeira posição\n",
    "result_df = result_df[['dsc_item', 'melhor_classif', 'score', 'ID_classif', 'ID_dsc_item']]\n",
    "\n",
    "# Salvar o DataFrame resultante em um novo CSV\n",
    "result_df.to_csv('resultado.csv', index=False)\n",
    "\n",
    "# Exibir as primeiras linhas do DataFrame resultante\n",
    "print(result_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b452cde5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
